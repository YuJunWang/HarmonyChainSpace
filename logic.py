import os
import base64
import urllib.parse
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from google.api_core.exceptions import ResourceExhausted

# å¼•å…¥å„å¤§æ¨¡å‹åº«
from langchain_groq import ChatGroq
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

# ===========================
# 1. çŸ¥è­˜åº«
# ===========================
class KnowledgeBase:
    def __init__(self, persist_dir="./chroma_db"):
        self.persist_dir = persist_dir
        self.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        self.vector_db = None

    def load_db(self):
        if os.path.exists(self.persist_dir) and os.path.isdir(self.persist_dir):
            try:
                self.vector_db = Chroma(persist_directory=self.persist_dir, embedding_function=self.embeddings)
                return True
            except Exception as e:
                print(f"âŒ è³‡æ–™åº«è¼‰å…¥å¤±æ•—: {e}")
                return False
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°è·¯å¾‘ {self.persist_dir}ï¼Œè«‹å…ˆåŸ·è¡Œè³‡æ–™é‡å»ºè…³æœ¬ï¼")
            return False

    def get_context(self):
        if not self.vector_db: 
            success = self.load_db()
            if not success: return None
        return self.vector_db.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 4, "fetch_k": 20} 
        )

# ===========================
# 2. å®šç¾©è¼¸å‡ºçµæ§‹
# ===========================
class MediatorOutput(BaseModel):
    verdict: str = Field(description="å”èª¿è€…çš„æœ€çµ‚æŠ˜è¡·æ–¹æ¡ˆèˆ‡å»ºè­°ï¼Œç´”æ–‡å­—")
    design_prompt: str = Field(description="çµ¦ AI ç¹ªåœ–æ¨¡å‹çš„è‹±æ–‡ Prompt")

# ===========================
# 3. å¤šä»£ç†äººæ ¸å¿ƒé‚è¼¯
# ===========================
class LogicCore:
    def __init__(self):
        self.kb = KnowledgeBase()
        self.kb.load_db()

    # --- ğŸ­ LLM å·¥å»  ---
    def _create_llm(self, provider, api_key, model_name=None):
        if not api_key:
            raise ValueError(f"è«‹è¼¸å…¥ {provider} çš„ API Key")

        if provider == "Groq":
            return ChatGroq(
                groq_api_key=api_key, 
                model_name=model_name or "llama-3.3-70b-versatile",
                temperature=0.5
            )
        elif provider == "OpenAI":
            return ChatOpenAI(
                api_key=api_key,
                model=model_name or "gpt-4o",
                temperature=0.5
            )
        elif provider == "Gemini":
            return ChatGoogleGenerativeAI(
                google_api_key=api_key,
                model=model_name or "gemini-2.5-flash",
                temperature=0.5
            )
        else:
            raise ValueError("ä¸æ”¯æ´çš„æ¨¡å‹ä¾›æ‡‰å•†")

    # --- ğŸ“š å…±ç”¨åŠŸèƒ½ï¼šRAG æª¢ç´¢ ---
    def get_rag_context(self, query):
        retriever = self.kb.get_context()
        docs = retriever.invoke(query) if retriever else []
        context_text = "\n".join([d.page_content for d in docs])
        return context_text

    # --- ğŸ‘ï¸ è¦–è¦ºä»£ç†äºº ---
    def analyze_image(self, image_bytes, api_key, mime_type="image/jpeg"):
        if not api_key: return "æœªæä¾› Gemini Keyï¼Œç„¡æ³•åˆ†æåœ–ç‰‡ã€‚"
        
        vision_model = ChatGoogleGenerativeAI(
            model="gemini-3.0-pro", 
            temperature=0.7, 
            google_api_key=api_key
        )
        b64_string = base64.b64encode(image_bytes).decode("utf-8")
        image_url = f"data:{mime_type};base64,{b64_string}"
        
        prompt = "è«‹ä»”ç´°è§€å¯Ÿé€™å¼µå®¤å…§è¨­è¨ˆåœ–ç‰‡ã€‚æè¿°ç©ºé–“ä½ˆå±€ï¼Œé‡é»åŒ…å«ï¼šçª—æˆ¶ä½ç½®ã€æ¨‘æŸ±ä½ç½®ã€é–€çš„ç›¸å°ä½ç½®ã€‚è«‹ç”¨å®¢è§€çš„ç´”æ–‡å­—æè¿°ã€‚"
        message = [
            {
                "role": "user", 
                "content": [
                    {"type": "text", "text": prompt}, 
                    {"type": "image_url", "image_url": image_url}
                ]
            }
        ]
        
        response = vision_model.invoke(message)
        return response.content

    # --- ğŸ—ï¸ Agent 1: å»ºç¯‰ç§‘å­¸å®¶ ---
    def run_architect_agent(self, context, query, provider, api_key, model_name):
        llm = self._create_llm(provider, api_key, model_name)
        
        template = """ä½ ç¾åœ¨æ˜¯ã€å°ˆæ¥­çš„å»ºç¯‰ç§‘å­¸ç ”ç©¶å“¡ã€‘ã€‚è«‹æ ¹æ“šã€åƒè€ƒè³‡æ–™ã€‘èˆ‡ã€ä½¿ç”¨è€…éœ€æ±‚ã€‘ï¼Œé€²è¡Œç§‘å­¸æ€§çš„ç©ºé–“æª¢è¨ã€‚
        
        ã€åƒè€ƒè³‡æ–™ã€‘ï¼š{context}
        ã€ä½¿ç”¨è€…éœ€æ±‚ã€‘ï¼š{query}
        
        è«‹åŸ·è¡Œä»¥ä¸‹æ€è€ƒæ­¥é©Ÿï¼š
        1. **è¨­è¨ˆæ„åœ–åˆ†æ**ï¼šç†è§£ä½¿ç”¨è€…æƒ³åšä»€éº¼ã€‚
        2. **ç§‘å­¸æ€§çš„åˆ¤æ–·**ï¼š
           - æ€è€ƒæ”¹å‹•å°ç©ºé–“é€ æˆçš„ç‰©ç†çµæœï¼ˆæ²¹ç…™ã€æ¿•æ°£ã€é€šé¢¨ï¼‰ã€‚
           - æª¢æŸ¥ã€åƒè€ƒè³‡æ–™ã€‘ï¼Œåƒ…å¼•ç”¨ã€Œç›´æ¥ç›¸é—œã€æ¢æ–‡ã€‚
           - âš ï¸ é‡è¦ï¼šè‹¥åƒè€ƒè³‡æ–™ç„¡é—œï¼Œè«‹å¿½ç•¥ä¸¦ä»¥ç§‘å­¸å¸¸è­˜å›ç­”ã€‚
        
        3. **è¼¸å‡ºè¦æ±‚**ï¼šè«‹è¼¸å‡º HTML æ ¼å¼ï¼ŒåŒ…å«ï¼š
           - `<p><b>ğŸ“ è¨­è¨ˆæ„åœ–ï¼š</b>...</p>`
           - `<p><b>ğŸ” ç§‘å­¸æª¢è¦–èˆ‡æ³•è¦ï¼š</b>...</p>`
           - `<ul><li><b>å¼•ç”¨ä¾æ“šï¼š</b>(ç›´æ¥å¯«å‡ºç›¸é—œæ³•æ¢ï¼Œè‹¥ç„¡RAGè³‡æ–™å‰‡å¯«"ä¾æ“šä¸€èˆ¬å»ºç¯‰å¸¸è¦")</li><li><b>å…·é«”å»ºè­°ï¼š</b>...</li></ul>`
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm | StrOutputParser()
        return chain.invoke({"context": context, "query": query})

    # --- ğŸ”® Agent 2: é¢¨æ°´å¤§å¸« ---
    def run_fengshui_agent(self, context, query, provider, api_key, model_name):
        llm = self._create_llm(provider, api_key, model_name)
        
        template = """ä½ ç¾åœ¨æ˜¯ã€è³‡æ·±é¢¨æ°´å¤§å¸«ã€‘ã€‚è«‹æ ¹æ“šã€åƒè€ƒè³‡æ–™ã€‘èˆ‡ã€ä½¿ç”¨è€…éœ€æ±‚ã€‘ï¼Œé€²è¡Œæ ¼å±€å‰å‡¶è¨ºæ–·ã€‚
        
        ã€åƒè€ƒè³‡æ–™ã€‘ï¼š{context}
        ã€ä½¿ç”¨è€…éœ€æ±‚ã€‘ï¼š{query}
        
        è«‹åŸ·è¡Œä»¥ä¸‹æ€è€ƒæ­¥é©Ÿï¼š
        1. **ç…æ°£è¨ºæ–·**ï¼šåˆ¤æ–·æ ¼å±€æ¶‰åŠå“ªç¨®å…·é«”ç¦å¿Œã€‚
        2. **å¤ç±æ¯”å°**ï¼š
           - æª¢æŸ¥ã€åƒè€ƒè³‡æ–™ã€‘æ˜¯å¦æœ‰å°æ‡‰åŸæ–‡ã€‚
           - âš ï¸ é‡è¦ï¼šè‹¥åƒè€ƒè³‡æ–™ç„¡é—œï¼Œè«‹å¿½ç•¥ä¸¦ä»¥å…§å»ºé¢¨æ°´çŸ¥è­˜å›ç­”ã€‚
        
        3. **è¼¸å‡ºè¦æ±‚**ï¼šè«‹è¼¸å‡º HTML æ ¼å¼ï¼ŒåŒ…å«ï¼š
           - `<p><b>ğŸ”® é™½å®…æ ¼å±€è¨ºæ–·ï¼š</b>...</p>`
           - `<p><b>ğŸ“œ å¤ç±èˆ‡æ°‘ä¿—è§€é»ï¼š</b>...</p>`
           - `<ul><li><b>ç¶“å…¸å¼•æ“šï¼š</b>(ç›´æ¥å¯«å‡ºç¶“å…¸åç¨±ï¼Œè‹¥ç„¡RAGè³‡æ–™å‰‡å¯«ä¾æ“šä¸€èˆ¬é¢¨æ°´è¦‹è§£)...</li><li><b>åŒ–è§£ä¹‹é“ï¼š</b>...</li></ul>`
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm | StrOutputParser()
        return chain.invoke({"context": context, "query": query})

    # --- ğŸ¤ Agent 3: å”èª¿è¨­è¨ˆå¸« ---
    def run_mediator_agent(self, arch_res, fs_res, query, style, provider, api_key, model_name):
        llm = self._create_llm(provider, api_key, model_name)
        parser = JsonOutputParser(pydantic_object=MediatorOutput)
        
        template = """ä½ æ˜¯ä¸€å€‹å¤šå·¥è™•ç†çš„å°ˆå®¶ï¼Œç¾åœ¨åŒæ™‚æ“”ä»»ã€å®¶åº­è£ä¿®å”èª¿è€…ã€‘èˆ‡ã€AI ç¹ªåœ–è© å”±å¸«ã€‘ã€‚
        
        ã€ä½¿ç”¨è€…éœ€æ±‚ã€‘ï¼š{query}
        ã€é¢¨æ ¼åå¥½ã€‘ï¼š{style}
        
        ã€å»ºç¯‰å¸«æ„è¦‹ã€‘ï¼š{arch_res}
        ã€é¢¨æ°´å¸«æ„è¦‹ã€‘ï¼š{fs_res}
        
        è«‹å®Œæˆä»¥ä¸‹å…©é …ä»»å‹™ä¸¦è¼¸å‡º JSONï¼š

        ### ä»»å‹™ 1ï¼šå”èª¿èˆ‡æŠ˜è¡· (Verdict)
        1. ç¶œåˆä¸Šè¿°å…©ä½çš„è§€é»ï¼ŒæŒ‡å‡ºè¡çªé»ï¼Œæˆ–æ˜¯çµ„åˆé›™æ–¹æå‡ºçš„å…±è­˜ã€‚
        2. æä¾›ä¸€å€‹ã€Œå…·é«”æŠ˜è¡·æ–¹æ¡ˆã€ï¼ŒåŒæ™‚æ»¿è¶³ç§‘å­¸ï¼ˆé€šé¢¨/æ¡å…‰ï¼‰èˆ‡é¢¨æ°´ï¼ˆå¿ƒç†/é¿ç…ï¼‰ã€‚
        3. è¼¸å‡ºç´”æ–‡å­—ï¼Œèªæ°£æº«å’Œå°ˆæ¥­ã€‚

        ### ä»»å‹™ 2ï¼šè¦–è¦ºåŒ– Prompt ç”Ÿæˆ (Design Prompt)
        æ ¹æ“šä½ çš„æŠ˜è¡·æ–¹æ¡ˆèˆ‡ä½¿ç”¨è€…çš„ã€Œ{style}ã€é¢¨æ ¼ï¼Œæ’°å¯«ä¸€å€‹è‹±æ–‡ Promptã€‚
        **é—œéµè¦æ±‚ï¼š**
        - **é‡å° FLUX æ¨¡å‹å„ªåŒ–**
        - **è¦–è§’è¨­å®š**ï¼šä½¿ç”¨ "High-angle 3/4 perspective view" åŠ "Cutaway 3D render"ã€‚
        - **æ§‹åœ–ç›®æ¨™**ï¼šç•«é¢ç¨å¾®æ—‹è½‰ï¼Œä¸è¦å®Œå…¨æ­£å°ç‰†é¢ã€‚
        - åŒ…å«é¢¨æ ¼é—œéµå­— (e.g., photorealistic, 8k)ã€‚
        - é¿å…ä¸ç¬¦åˆç¾å¯¦çš„æ™¯è±¡ã€‚
        
        {format_instructions}
        """
        
        prompt = ChatPromptTemplate.from_template(template, partial_variables={"format_instructions": parser.get_format_instructions()})
        chain = prompt | llm | parser
        return chain.invoke({"arch_res": arch_res, "fs_res": fs_res, "query": query, "style": style})

    # --- ğŸ¨ ç¹ªåœ–å·¥å…· ---
    def generate_image_via_pollinations(self, prompt):
        """
        ä½¿ç”¨ Pollinations.ai å…è²»ç”Ÿæˆåœ–ç‰‡
        """
        import urllib.parse
        import random  # æ–°å¢ random ä»¥ç”Ÿæˆéš¨æ©Ÿç¨®å­
        
        try:
            # 1. å° Prompt é€²è¡Œ URL ç·¨ç¢¼ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦å°è‡´é€£çµå¤±æ•ˆ
            encoded_prompt = urllib.parse.quote(prompt)
            
            # 2. ç”Ÿæˆéš¨æ©Ÿç¨®å­ (Seed)ï¼Œç¢ºä¿æ¯æ¬¡ç”Ÿæˆçš„æ§‹åœ–éƒ½ä¸ä¸€æ¨£
            seed = random.randint(0, 99999)
            
            # 3. å»ºç«‹ URL
            # model=flux-realism: æŒ‡å®š 2025 æ›´å¼·çš„çœŸå¯¦æ„Ÿæ¨¡å‹
            # seed={seed}: å›ºå®šéš¨æ©Ÿæ€§
            # nologo=true: å˜—è©¦éš±è—æµ®æ°´å°
            url = f"https://image.pollinations.ai/prompt/{encoded_prompt}?model=flux-realism&width=1024&height=768&seed={seed}&nologo=true"
            
            return url 
        except Exception as e:
            print(f"Pollinations Error: {e}")
            return None

    def generate_image_from_hf(self, prompt, hf_token):
        if not hf_token: return None
        from huggingface_hub import InferenceClient
        client = InferenceClient(model="black-forest-labs/FLUX.1-schnell", token=hf_token)
        try:
            image = client.text_to_image(prompt)
            return image
        except Exception as e:
            print(f"HF Error: {e}")
            return None
