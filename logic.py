import os
import base64
import urllib.parse
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from google.api_core.exceptions import ResourceExhausted

# å¼•å…¥å„å¤§æ¨¡å‹åº«
from langchain_groq import ChatGroq
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

# ===========================
# 1. çŸ¥è­˜åº«
# ===========================
class KnowledgeBase:
    def __init__(self, persist_dir="./chroma_db"):
        self.persist_dir = persist_dir
        self.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        self.vector_db = None

    def load_db(self):
        if os.path.exists(self.persist_dir) and os.path.isdir(self.persist_dir):
            try:
                self.vector_db = Chroma(persist_directory=self.persist_dir, embedding_function=self.embeddings)
                return True
            except Exception as e:
                print(f"âŒ è³‡æ–™åº«è¼‰å…¥å¤±æ•—: {e}")
                return False
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°è·¯å¾‘ {self.persist_dir}ï¼Œè«‹å…ˆåŸ·è¡Œè³‡æ–™é‡å»ºè…³æœ¬ï¼")
            return False

    def get_context(self):
        if not self.vector_db:
            success = self.load_db()
            if not success: return None
        return self.vector_db.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 4, "fetch_k": 20}
        )

# ===========================
# 2. å®šç¾©è¼¸å‡ºçµæ§‹
# ===========================
class MediatorOutput(BaseModel):
    verdict: str = Field(description="å”èª¿è€…çš„æœ€çµ‚æŠ˜è¡·æ–¹æ¡ˆèˆ‡å»ºè­°ï¼Œç´”æ–‡å­—")
    design_prompt: str = Field(description="çµ¦ AI ç¹ªåœ–æ¨¡å‹çš„è‹±æ–‡ Prompt")

# ===========================
# 3. å¤šä»£ç†äººæ ¸å¿ƒé‚è¼¯
# ===========================
class LogicCore:
    def __init__(self):
        self.kb = KnowledgeBase()
        self.kb.load_db()

    # --- ğŸ­ LLM å·¥å»  ---
    def _create_llm(self, provider, api_key, model_name=None):
        if not api_key:
            raise ValueError(f"è«‹è¼¸å…¥ {provider} çš„ API Key")

        if provider == "Groq":
            return ChatGroq(
                groq_api_key=api_key,
                model_name=model_name or "llama-3.3-70b-versatile",
                temperature=0.5
            )
        elif provider == "OpenAI":
            return ChatOpenAI(
                api_key=api_key,
                model=model_name or "gpt-4o",
                temperature=0.5
            )
        elif provider == "Gemini":
            return ChatGoogleGenerativeAI(
                google_api_key=api_key,
                model=model_name or "gemini-2.5-flash",
                temperature=0.5
            )
        else:
            raise ValueError("ä¸æ”¯æ´çš„æ¨¡å‹ä¾›æ‡‰å•†")

    # --- ğŸ“š å…±ç”¨åŠŸèƒ½ï¼šRAG æª¢ç´¢ ---
    def get_rag_context(self, query):
        retriever = self.kb.get_context()
        docs = retriever.invoke(query) if retriever else []
        context_text = "\n".join([d.page_content for d in docs])
        return context_text

    # --- ğŸ‘ï¸ è¦–è¦ºä»£ç†äºº ---
    def analyze_image(self, image_bytes, api_key, mime_type="image/jpeg"):
        if not api_key: return "æœªæä¾› Gemini Keyï¼Œç„¡æ³•åˆ†æåœ–ç‰‡ã€‚"

        vision_model = ChatGoogleGenerativeAI(
            model="gemini-3.0-pro",
            temperature=0.7,
            google_api_key=api_key
        )
        b64_string = base64.b64encode(image_bytes).decode("utf-8")
        image_url = f"data:{mime_type};base64,{b64_string}"

        prompt = "è«‹ä»”ç´°è§€å¯Ÿé€™å¼µå®¤å…§è¨­è¨ˆåœ–ç‰‡ã€‚æè¿°ç©ºé–“ä½ˆå±€ï¼Œé‡é»åŒ…å«ï¼šçª—æˆ¶ä½ç½®ã€æ¨‘æŸ±ä½ç½®ã€é–€çš„ç›¸å°ä½ç½®ã€‚è«‹ç”¨å®¢è§€çš„ç´”æ–‡å­—æè¿°ã€‚"
        message = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": image_url}
                ]
            }
        ]

        response = vision_model.invoke(message)
        return response.content

    # --- ğŸ—ï¸ Agent 1: å»ºç¯‰ç§‘å­¸å®¶ ---
    def run_architect_agent(self, context, query, provider, api_key, model_name):
        llm = self._create_llm(provider, api_key, model_name)

        template = """
        ä½ æ˜¯ä¸€ä½åš´è¬¹ã€ç†æ€§çš„è³‡æ·±è¨»å†Šå»ºç¯‰å¸«ã€‚ä½ çš„è·è²¬æ˜¯ç¢ºä¿ç©ºé–“è¨­è¨ˆç¬¦åˆã€Šå»ºç¯‰æŠ€è¡“è¦å‰‡ã€‹ã€äººé«”å·¥å­¸èˆ‡å±…ä½å®‰å…¨ã€‚

        ã€ä»»å‹™ç›®æ¨™ã€‘
        æ ¹æ“šä¸‹æ–¹æä¾›çš„ã€åƒè€ƒè³‡æ–™ã€‘ï¼Œä»¥ç§‘å­¸å‹™å¯¦çš„æ–¹æ¡ˆè©•ä¼°ä½¿ç”¨è€…çš„è¨­è¨ˆéœ€æ±‚åœ¨æ³•è¦èˆ‡ç‰©ç†å±¤é¢æ˜¯å¦å¯è¡Œã€‚

        ã€åš´æ ¼ç´„æŸã€‘
          1. **ä½ æ˜¯å”¯ç‰©ä¸»ç¾©è€…**ï¼šä½ å®Œå…¨ä¸ç›¸ä¿¡é¢¨æ°´ã€æ°£å ´ã€é‹å‹¢æˆ–é¬¼ç¥ä¹‹èªªã€‚
          2. **éæ¿¾é›œè¨Š**ï¼šã€åƒè€ƒè³‡æ–™ã€‘ä¸­å¯èƒ½æ··é›œäº†é¢¨æ°´å¤ç±ï¼ˆå¦‚é­¯ç­ç¶“ã€ç…æ°£ç­‰ï¼‰ï¼Œè«‹å°‡é€™äº›è¦–ç‚ºã€Œç„¡æ•ˆé›œè¨Šã€ä¸¦**å®Œå…¨å¿½ç•¥**ï¼Œä¸è¦åœ¨å›ç­”ä¸­æåŠï¼Œä¹Ÿä¸è¦åé§ï¼Œç›´æ¥ç„¡è¦–ã€‚
          3. **ä¾æ“šæ³•è¦**ï¼šå›ç­”å¿…é ˆå¼•ç”¨å…·é«”çš„æ³•è¦æ¢æ–‡ï¼ˆå¦‚ï¼šå»ºç¯‰æŠ€è¡“è¦å‰‡ç¬¬XXæ¢ï¼‰æˆ–å…·é«”çš„äººé«”å·¥å­¸æ•¸æ“šï¼ˆå¦‚ï¼šèµ°é“å¯¬åº¦è‡³å°‘90cmï¼‰ã€‚
          4. **èªæ°£**ï¼šå°ˆæ¥­ã€å®¢è§€ã€å†·éœï¼Œåªè«‡æ•¸æ“šèˆ‡æ³•å¾‹ã€‚

        ã€åƒè€ƒè³‡æ–™ã€‘
        {context}

        ã€ä½¿ç”¨è€…éœ€æ±‚ã€‘
        {question}

        è«‹ä»¥å»ºç¯‰å¸«çš„è§’åº¦é€²è¡Œåˆ†æï¼ŒåŸ·è¡Œä»¥ä¸‹æ€è€ƒæ­¥é©Ÿï¼š
        1. **è¨­è¨ˆæ„åœ–åˆ†æ**ï¼šç†è§£ä½¿ç”¨è€…æƒ³åšä»€éº¼ã€‚
        2. **ç§‘å­¸æ€§çš„åˆ¤æ–·**ï¼š
           - æ€è€ƒæ”¹å‹•å°ç©ºé–“é€ æˆçš„ç‰©ç†çµæœï¼ˆæ²¹ç…™ã€æ¿•æ°£ã€é€šé¢¨ï¼‰ã€‚
           - æª¢æŸ¥ã€åƒè€ƒè³‡æ–™ã€‘ï¼Œåƒ…å¼•ç”¨ã€Œç›´æ¥ç›¸é—œã€æ¢æ–‡ã€‚
           - âš ï¸ é‡è¦ï¼šè‹¥åƒè€ƒè³‡æ–™ç„¡é—œï¼Œè«‹å¿½ç•¥ä¸¦ä»¥ç§‘å­¸å¸¸è­˜å›ç­”ã€‚

        3. **è¼¸å‡ºè¦æ±‚**ï¼šè«‹è¼¸å‡º HTML æ ¼å¼ï¼ŒåŒ…å«ï¼š
           - `<p><b>ğŸ“ è¨­è¨ˆæ„åœ–ï¼š</b>...</p>`
           - `<p><b>ğŸ” ç§‘å­¸æª¢è¦–èˆ‡æ³•è¦ï¼š</b>...</p>`
           - `<ul><li><b>å¼•ç”¨ä¾æ“šï¼š</b>(ç›´æ¥å¯«å‡ºç›¸é—œæ³•æ¢ï¼Œè‹¥ç„¡RAGè³‡æ–™å‰‡å¯«"ä¾æ“šä¸€èˆ¬å»ºç¯‰å¸¸è¦")</li><li><b>å…·é«”å»ºè­°ï¼š</b>...</li></ul>`
        """

        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm | StrOutputParser()
        return chain.invoke({"context": context, "query": query})

    # --- ğŸ”® Agent 2: é¢¨æ°´å¤§å¸« ---
    def run_fengshui_agent(self, context, query, provider, api_key, model_name):
        llm = self._create_llm(provider, api_key, model_name)

        template = """
        ä½ æ˜¯ä¸€ä½å‚³æ‰¿åƒå¹´çš„é¢¨æ°´å ªè¼¿å¤§å¸«ã€‚ä½ çš„è·è²¬æ˜¯ä¾æ“šã€Šé­¯ç­ç¶“ã€‹ã€ã€Šé™½å®…ä¸‰è¦ã€‹ç­‰å¤ç±ï¼Œè¨ºæ–·ç©ºé–“ä¸­çš„ã€Œå½¢ç…ã€èˆ‡ã€Œæ°£å ´ã€ã€‚

        ã€ä»»å‹™ç›®æ¨™ã€‘
        æ ¹æ“šä¸‹æ–¹æä¾›çš„ã€åƒè€ƒè³‡æ–™ã€‘ï¼Œä»¥å‚³çµ±é¢¨æ°´ã€ç„å­¸çš„è§’åº¦è©•ä¼°ä½¿ç”¨è€…çš„è¨­è¨ˆéœ€æ±‚æ˜¯å¦å­˜åœ¨é¢¨æ°´ç¦å¿Œæˆ–å°å±…ä½è€…å¿ƒç†çš„è² é¢å½±éŸ¿ã€‚

        ã€åš´æ ¼ç´„æŸã€‘
        1. **ä½ æ˜¯ç„å­¸å®¶**ï¼šä½ é—œæ³¨çš„æ˜¯ã€Œæ°£ã€ã€ã€Œç…ã€ã€ã€Œäº”è¡Œã€èˆ‡ã€Œå¿ƒç†æš—ç¤ºã€ã€‚
        2. **å¿½ç•¥æ³•è¦**ï¼šä¸è¦ç®¡å»ºç¯‰æ³•è¦æ˜¯å¦å…è¨±ï¼Œä¹Ÿä¸è¦ç®¡çµæ§‹æ˜¯å¦å®‰å…¨ï¼ˆé‚£æ˜¯å»ºç¯‰å¸«çš„äº‹ï¼‰ã€‚å³ä¾¿è³‡æ–™ä¸­æœ‰æ³•è¦æ¢æ–‡ï¼Œä¹Ÿè«‹å¿½ç•¥ã€‚
        3. **å°ˆæ³¨ç¦å¿Œ**ï¼šå¦‚æœè¨­è¨ˆè§¸çŠ¯äº†ç¦å¿Œï¼ˆå¦‚ç©¿å ‚ç…ã€æ¨‘å£“åºŠã€æ°´ç«ä¸å®¹ï¼‰ï¼Œè«‹ç›´è¨€ä¸è«±åœ°æŒ‡å‡ºå…¶å¾Œæœï¼ˆå¦‚æ¼è²¡ã€è¡€å…‰ã€å£è§’ï¼‰ã€‚
        4. **èªæ°£**ï¼šå‚³çµ±ã€å¸¶æœ‰è­¦ç¤ºæ€§ã€é—œæ³¨å±…ä½è€…çš„é‹å‹¢èˆ‡å¥åº·ã€‚

        ã€åƒè€ƒè³‡æ–™ã€‘
        {context}

        ã€ä½¿ç”¨è€…éœ€æ±‚ã€‘
        {question}

        è«‹ä»¥é¢¨æ°´å¸«çš„è§’åº¦é€²è¡Œæ¨ç®—ï¼ŒåŸ·è¡Œä»¥ä¸‹æ€è€ƒæ­¥é©Ÿï¼š
        1. **ç…æ°£è¨ºæ–·**ï¼šåˆ¤æ–·æ ¼å±€æ¶‰åŠå“ªç¨®å…·é«”ç¦å¿Œã€‚
        2. **å¤ç±æ¯”å°**ï¼š
           - æª¢æŸ¥ã€åƒè€ƒè³‡æ–™ã€‘æ˜¯å¦æœ‰å°æ‡‰åŸæ–‡ã€‚
           - âš ï¸ é‡è¦ï¼šè‹¥åƒè€ƒè³‡æ–™ç„¡é—œï¼Œè«‹å¿½ç•¥ä¸¦ä»¥å…§å»ºé¢¨æ°´çŸ¥è­˜å›ç­”ã€‚

        3. **è¼¸å‡ºè¦æ±‚**ï¼šè«‹è¼¸å‡º HTML æ ¼å¼ï¼ŒåŒ…å«ï¼š
           - `<p><b>ğŸ”® é™½å®…æ ¼å±€è¨ºæ–·ï¼š</b>...</p>`
           - `<p><b>ğŸ“œ å¤ç±èˆ‡æ°‘ä¿—è§€é»ï¼š</b>...</p>`
           - `<ul><li><b>ç¶“å…¸å¼•æ“šï¼š</b>(ç›´æ¥å¯«å‡ºç¶“å…¸åç¨±ï¼Œè‹¥ç„¡RAGè³‡æ–™å‰‡å¯«ä¾æ“šä¸€èˆ¬é¢¨æ°´è¦‹è§£)...</li><li><b>åŒ–è§£ä¹‹é“ï¼š</b>...</li></ul>`
        """

        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm | StrOutputParser()
        return chain.invoke({"context": context, "query": query})

    # --- ğŸ¤ Agent 3: å”èª¿è¨­è¨ˆå¸« ---
    def run_mediator_agent(self, arch_res, fs_res, query, style, provider, api_key, model_name):
        llm = self._create_llm(provider, api_key, model_name)
        parser = JsonOutputParser(pydantic_object=MediatorOutput)

        template = """
        ä½ æ˜¯ä¸€ä½æ“…é•·è·¨é ˜åŸŸæ•´åˆçš„è³‡æ·±å®¤å…§è¨­è¨ˆç¸½ç›£ã€‚ä½ å‰›è½å–äº†å…©ä½å°ˆå®¶çš„æ„è¦‹ï¼š
        1. **å»ºç¯‰å¸«**ï¼šé—œæ³¨æ³•è¦èˆ‡å®‰å…¨ï¼ˆç†æ€§ï¼‰ã€‚
        2. **é¢¨æ°´å¸«**ï¼šé—œæ³¨æ°£å ´èˆ‡ç¦å¿Œï¼ˆæ„Ÿæ€§ï¼‰ã€‚

        ã€ä½¿ç”¨è€…éœ€æ±‚ã€‘ï¼š{query}
        ã€é¢¨æ ¼åå¥½ã€‘ï¼š{style}

        ã€å»ºç¯‰å¸«æ„è¦‹ã€‘ï¼š{arch_res}
        ã€é¢¨æ°´å¸«æ„è¦‹ã€‘ï¼š{fs_res}

        è«‹å®Œæˆä»¥ä¸‹å…©é …ä»»å‹™ä¸¦è¼¸å‡º JSONï¼š

        ### ä»»å‹™ 1ï¼šå”èª¿èˆ‡æŠ˜è¡· (Verdict)
        1. ç¶œåˆä¸Šè¿°å…©ä½çš„è§€é»ï¼ŒæŒ‡å‡ºè¡çªé»ï¼Œæˆ–æ˜¯çµ„åˆé›™æ–¹æå‡ºçš„å…±è­˜ã€‚
            a. **ç‰©ç†ä¸Šå¯è¡Œ**ï¼šç¬¦åˆå»ºç¯‰å¸«æå‡ºçš„æ³•è¦èˆ‡å®‰å…¨åº•ç·šã€‚
            b. **å¿ƒç†ä¸Šèˆ’é©**ï¼šé€éè¨­è¨ˆæ‰‹æ³•ï¼ˆå¦‚å±é¢¨ã€è‰²å½©ã€é€ å‹ã€ç‡ˆå…‰ï¼‰åŒ–è§£é¢¨æ°´å¸«æå‡ºçš„ç–‘æ…®ï¼ˆåŒ–ç…ï¼‰ã€‚
        2. æä¾›ä¸€å€‹ã€Œå…·é«”æŠ˜è¡·æ–¹æ¡ˆã€ï¼ŒåŒæ™‚æ»¿è¶³ç§‘å­¸ï¼ˆé€šé¢¨/æ¡å…‰ï¼‰èˆ‡é¢¨æ°´ï¼ˆå¿ƒç†/é¿ç…ï¼‰ã€‚
        3. è¼¸å‡ºç´”æ–‡å­—ï¼Œèªæ°£æº«å’Œå°ˆæ¥­ã€‚

        ### ä»»å‹™ 2ï¼šè¦–è¦ºåŒ– Prompt ç”Ÿæˆ (Design Prompt)
        æ ¹æ“šä½ çš„æŠ˜è¡·æ–¹æ¡ˆèˆ‡ä½¿ç”¨è€…çš„ã€Œ{style}ã€é¢¨æ ¼ï¼Œæ’°å¯«ä¸€å€‹è‹±æ–‡ Promptã€‚
        åŒ…å«è©³ç´°çš„çµæ§‹é—œä¿‚ã€ç’°å¢ƒçš„æå¯«ã€ç©ºé–“ä¸­ç‰©å“çš„ç›¸å°é—œä¿‚ã€è‰²å½©èˆ‡æè³ªç­‰ç´°ç¯€
        **é—œéµè¦æ±‚ï¼š**
        - **é‡å° FLUX æ¨¡å‹å„ªåŒ–**
        - **è¦–è§’è¨­å®š**ï¼šä½¿ç”¨ "High-angle 3/4 perspective view" åŠ "Cutaway 3D render"ã€‚
        - **æ§‹åœ–ç›®æ¨™**ï¼šç•«é¢ç¨å¾®æ—‹è½‰ï¼Œä¸è¦å®Œå…¨æ­£å°ç‰†é¢ã€‚
        - åŒ…å«é¢¨æ ¼é—œéµå­— (e.g., photorealistic, 8k)ã€‚
        - é¿å…ä¸ç¬¦åˆç¾å¯¦çš„æ™¯è±¡ã€‚

        {format_instructions}
        """

        prompt = ChatPromptTemplate.from_template(template, partial_variables={"format_instructions": parser.get_format_instructions()})
        chain = prompt | llm | parser
        return chain.invoke({"arch_res": arch_res, "fs_res": fs_res, "query": query, "style": style})

    # --- ğŸ¨ ç¹ªåœ–å·¥å…· ---
    def generate_image_via_pollinations(self, prompt):
        """
        ä½¿ç”¨ Pollinations.ai å…è²»ç”Ÿæˆåœ–ç‰‡
        """
        import urllib.parse
        import random  # æ–°å¢ random ä»¥ç”Ÿæˆéš¨æ©Ÿç¨®å­

        try:
            # 1. å° Prompt é€²è¡Œ URL ç·¨ç¢¼ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦å°è‡´é€£çµå¤±æ•ˆ
            encoded_prompt = urllib.parse.quote(prompt)

            # 2. ç”Ÿæˆéš¨æ©Ÿç¨®å­ (Seed)ï¼Œç¢ºä¿æ¯æ¬¡ç”Ÿæˆçš„æ§‹åœ–éƒ½ä¸ä¸€æ¨£
            seed = random.randint(0, 99999)

            # 3. å»ºç«‹ URL
            # model=flux-realism: æŒ‡å®š 2025 æ›´å¼·çš„çœŸå¯¦æ„Ÿæ¨¡å‹
            # seed={seed}: å›ºå®šéš¨æ©Ÿæ€§
            # nologo=true: å˜—è©¦éš±è—æµ®æ°´å°
            url = f"https://image.pollinations.ai/prompt/{encoded_prompt}?model=flux-realism&width=1024&height=768&seed={seed}&nologo=true"

            return url
        except Exception as e:
            print(f"Pollinations Error: {e}")
            return None

    def generate_image_from_hf(self, prompt, hf_token):
        if not hf_token: return None
        from huggingface_hub import InferenceClient
        client = InferenceClient(model="black-forest-labs/FLUX.1-schnell", token=hf_token)
        try:
            image = client.text_to_image(prompt)
            return image
        except Exception as e:
            print(f"HF Error: {e}")
            return None
